{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "RPJCdHDXAFxj"
      },
      "source": [
        "# Imports\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjUa9jXxARDe"
      },
      "source": [
        "image = tf.keras.preprocessing.image.load_img(\"portrait.jpg\" , target_size = (144,144))\n",
        "print(image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wLTIuQ9B4ud"
      },
      "source": [
        "imageArray = tf.keras.preprocessing.image.img_to_array(image)\n",
        "print(imageArray.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YS4vcnDFCO5e"
      },
      "source": [
        "#Because we need to have dimension in every patch so we add one column to our matrix\n",
        "imageArray = imageArray[tf.newaxis , ...]\n",
        "print(imageArray.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ah7DIM78DIQ1"
      },
      "source": [
        "#We need to  make some patches\n",
        "patches = tf.image.extract_patches(imageArray , sizes = [1,16,16,1] , strides = [1,16,16,1], rates = [1,1,1,1], padding = \"VALID\" )\n",
        "print(patches.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_lb7ltbIJ15"
      },
      "source": [
        "#Flatten the patches\n",
        "patches = tf.reshape(patches , shape=(tf.shape(patches)[0] , -1 , 16*16*3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_9AWPaUIsn3"
      },
      "source": [
        "print(patches.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZiFHvGI5I6AY"
      },
      "source": [
        "#Divide the image to some patches\n",
        "n = int(np.sqrt(patches.shape[1]))\n",
        "plt.figure(figsize=(n*n,1))\n",
        "for i, patch in enumerate(patches[0]):\n",
        "    ax = plt.subplot(1,n*n, i + 1)\n",
        "    patch_img = tf.reshape(patch, (16 , 16 , 3))\n",
        "    plt.imshow(patch_img.numpy().astype(\"uint8\"))\n",
        "    plt.axis(\"off\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTP4O0ZKPXPE"
      },
      "source": [
        "#DEFINING A  CLASS TO DO PATCH EMBEDDING AUTOMATICALLY\n",
        "class PatchEmbedding(tf.keras.layers.Layer):\n",
        "  def __init__(self, size , num_of_patches , projection_dim):\n",
        "    super().__init__()\n",
        "    self.size = size\n",
        "    #we add +1 because ClS has a position for himself\n",
        "    self.num_of_patches = num_of_patches + 1 \n",
        "    self.projection_dim = projection_dim\n",
        "    self.projection = tf.keras.layers.Dense(projection_dim)\n",
        "    self.clsToken = tf.Variable(tf.keras.initializers.GlorotNormal()(shape = (1,1,projection_dim)) , trainable = True)\n",
        "    self.positionalEmbedding = tf.keras.layers.Embedding(self.num_of_patches , projection_dim)\n",
        "\n",
        "\n",
        "  def call(self , inputs):\n",
        "    #extracting  patches\n",
        "    patches = tf.image.extract_patches(inputs, sizes=[1 , self.size, self.size , 1] , strides = [1, self.size , self.size , 1] , rates = [1 , 1 , 1 , 1] , padding = \"VALID\")\n",
        "    #make 1D patches. we know that the image is color\n",
        "    #if we don't know, we can change the code to be dynamic!\n",
        "    patches = tf .reshape(patches , (tf.shape(inputs)[0] , -1 , self.size * self.size * 3))\n",
        "    #project the patches with \"tf.keras.layers.Dense\"\n",
        "    patches = self.projection(patches)\n",
        "     \n",
        "    clsToken = tf.repeat(self.clsToken , tf.shape (inputs)[0], 0)\n",
        "    patches = tf.concat((clsToken , patches ), axis = 1)\n",
        "    #making positions with range. self.num_of_patches is number of positions\n",
        "    #and the third input is our step\n",
        "    positions = tf.range(0 , self.num_of_patches , 1)[tf.newaxis , ...]\n",
        "    #adding positions to vectors\n",
        "    positionalEmbedding = self.positionalEmbedding(positions)\n",
        "    #print(posisionalEmbedding)\n",
        "    patches = patches + positionalEmbedding\n",
        "    return patches"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iI_M6igoRIfm"
      },
      "source": [
        "embedding = PatchEmbedding(16,81,128)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gg8UEUshWQ9D"
      },
      "source": [
        "result = embedding (tf.random.normal(shape = (32,144,144,3)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j474GfhxWe2R"
      },
      "source": [
        "print(result.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFO4dxVKb9ET"
      },
      "source": [
        "#Transformer layer\n",
        "class TransformerLayer(tf.keras.layers.Layer):\n",
        "  def __init__ (self, d_model , heads , mlp_rate , dropout_rate = 0.1):\n",
        "     super().__init__()\n",
        "\n",
        "     self.layernorm_1 = tf.keras.layers.LayerNormalization(epsilon = 1e-6)\n",
        "     self.mha = tf.keras.layers.MultiHeadAttention(heads, d_model//heads , dropout = dropout_rate)\n",
        "      \n",
        "     self.layernorm_2 =tf.keras.layers.LayerNormalization(epsilon =1e-6)\n",
        "     self.mlp = tf.keras.Sequential([\n",
        "                                     tf.keras.layers.Dense(d_model * mlp_rate, activation=\"gelu\"),\n",
        "                                     tf.keras.layers.Dropout(dropout_rate),\n",
        "                                     tf.keras.layers.Dense(d_model),  # no activation on last layer\n",
        "                                     tf.keras.layers.Dropout(dropout_rate)\n",
        "      ])\n",
        "  def call(self, inputs, training = True):\n",
        "       out_1 = self.layernorm_1(inputs)\n",
        "       out_1 = self.mha(out_1, out_1, training = training)\n",
        "       out_1 = inputs + out_1\n",
        "\n",
        "       out_2 = self.layernorm_2(out_1)\n",
        "       out_2 = self.mlp(out_2, training = training)\n",
        "       out_2 = out_1 + out_2\n",
        "       return out_2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTkGn4eX4acq"
      },
      "source": [
        "#make a transformer encoder with transformer layers\n",
        "class TransformerEncoder(tf.keras.layers.Layer):\n",
        "  def __init__ (self, d_model , heads , mlp_rate , num_layers=1  , dropout_rate=0.1 ):\n",
        "     super().__init__()\n",
        "     self.encoders = [TransformerLayer(d_model , heads , mlp_rate , dropout_rate) for _ in range(num_layers)]\n",
        " \n",
        "  def call(self , inputs , training = True):\n",
        "    x = inputs\n",
        "\n",
        "    for layer in self.encoders:\n",
        "      x = layer(x, training = training)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBP2oerF9UGi"
      },
      "source": [
        "class ViT(tf.keras.Model):\n",
        "  def __init__(self, num_classes, patch_size, num_of_patches, d_model , heads , num_layers , mlp_rate , dropout_rate=0.1):\n",
        "     super().__init__()\n",
        "\n",
        "     self.patchEmbedding = PatchEmbedding(patch_size , num_of_patches , d_model)\n",
        "     self.encoder = TransformerEncoder(d_model , heads , mlp_rate ,num_layers, dropout_rate)\n",
        "     self.prediction = tf.keras.Sequential([\n",
        "                                           tf.keras.layers.Dropout(0.3),\n",
        "                                           tf.keras.layers.Dense(mlp_rate*d_model , activation = \"gelu\"), \n",
        "                                           tf.keras.layers.Dropout(0.3),\n",
        "                                           tf.keras.layers.Dense(num_classes , activation = \"softmax\")\n",
        "                                            \n",
        "     ])\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "  def call(self, inputs , training = True):\n",
        "     patches = self.patchEmbedding(inputs)\n",
        "     #print(patches)\n",
        "     encoderResult = self.encoder(patches , training = training)\n",
        "\n",
        "     clsResult = encoderResult[:,0,:]\n",
        "\n",
        "     prediction = self.prediction(clsResult,training=training)\n",
        "     return prediction"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIeDDw40zCBS"
      },
      "source": [
        "\n",
        "#To be careful about this section.  !!!!!!!\n",
        "vitClassifier = ViT(\n",
        "                    100,\n",
        "                    16,\n",
        "                    81,\n",
        "                    128,\n",
        "                    2,\n",
        "                    4,\n",
        "                    2,\n",
        "                    0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fr4-rtcUzXs-"
      },
      "source": [
        "vitClassifier(tf.random.normal(shape = (32 , 144 , 144 , 3)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hFQ5TT4VMwX"
      },
      "source": [
        "(x_train , y_train) , (x_test , y_test) = tf.keras.datasets.cifar10.load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YaiaMEn0Vgcf"
      },
      "source": [
        "print(x_train.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JyS7McVYVqcW"
      },
      "source": [
        "preprocessingModel = tf.keras.Sequential([\n",
        "                                          tf.keras.layers.Normalization(),\n",
        "                                          tf.keras.layers.Resizing(72,72),\n",
        "\n",
        "])\n",
        "preprocessingModel.layers[0].adapt(x_train)\n",
        "augmentationModel = tf.keras.Sequential([\n",
        "    tf.keras.layers.RandomFlip(\"horizontal\"),\n",
        "    tf.keras.layers.RandomRotation(0.2),\n",
        "    tf.keras.layers.RandomZoom(width_factor=0.2, height_factor=0.2),\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hk5LMGCTW9jg"
      },
      "source": [
        "def convert_to_dataset(data, batch_size, shuffle=False, augment=False):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(data)\n",
        "    dataset = dataset.map(\n",
        "        lambda x, y: (preprocessingModel(x), y),\n",
        "        num_parallel_calls=tf.data.AUTOTUNE,\n",
        "    )\n",
        "    if shuffle:\n",
        "        dataset = dataset.shuffle(len(dataset))\n",
        "    dataset = dataset.batch(batch_size, drop_remainder=True)\n",
        "    if augment:\n",
        "        dataset = dataset.map(\n",
        "            lambda x, y: (augmentationModel(x, training=True), y),\n",
        "            num_parallel_calls=tf.data.AUTOTUNE,\n",
        "        )\n",
        "    return dataset.prefetch(tf.data.AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-VwghUyZKPO"
      },
      "source": [
        "trainingData = convert_to_dataset((x_train , y_train) , 1024 , shuffle = True , augment=True)\n",
        "valData = convert_to_dataset ((x_test , y_test) , 1024 , shuffle = False , augment= False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WuZFW11fZAFw"
      },
      "source": [
        "#print(trainingData)\n",
        "#print(valData)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmzDM79aaQT4"
      },
      "source": [
        "# Use TPU if available (e.g. on Colab); otherwise GPU or CPU\n",
        "try:\n",
        "    resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=\"\")\n",
        "    tf.config.experimental_connect_to_cluster(resolver)\n",
        "    tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "    strategy = tf.distribute.TPUStrategy(resolver)\n",
        "    print(\"Using TPU:\", tf.config.list_logical_devices(\"TPU\"))\n",
        "except ValueError:\n",
        "    strategy = tf.distribute.MirroredStrategy() if tf.config.list_physical_devices(\"GPU\") else tf.distribute.get_strategy()\n",
        "    print(\"Using strategy:\", strategy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLE9JYxYbVvF"
      },
      "source": [
        "# Strategy already set in previous cell when using TPU"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_qDv1X4bg-u"
      },
      "source": [
        "trainingData = convert_to_dataset((x_train, y_train), 1024, shuffle=True, augment=True)\n",
        "valData = convert_to_dataset((x_test, y_test), 1024, shuffle=False, augment=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFZfhCawy3QV"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7cxBM0Nbo3V"
      },
      "source": [
        "with strategy.scope():\n",
        "    vitClassifier = ViT(\n",
        "        num_classes=10,\n",
        "        patch_size=6,\n",
        "        num_of_patches=(72 // 6) ** 2,\n",
        "        d_model=128,\n",
        "        heads=2,\n",
        "        num_layers=4,\n",
        "        mlp_rate=2,\n",
        "        dropout_rate=0.1,\n",
        "    )\n",
        "    vitClassifier.compile(\n",
        "        loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "        optimizer=\"adam\",\n",
        "        metrics=[\n",
        "            tf.keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n",
        "            tf.keras.metrics.SparseTopKCategoricalAccuracy(k=5, name=\"top_5_accuracy\"),\n",
        "        ],\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bj-JgBHm43lI"
      },
      "source": [
        "vitClassifier.fit(trainingData, validation_data=valData, epochs=20)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [],
      "name": "trasnformers1.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}